#!/bin/bash

# Enhanced Jackpot Analysis Orchestration Script
# Automates the complete workflow for 04/10/2025 jackpot predictions

set -e  # Exit on any error

# Configuration
TARGET_DATE="04/10/2025"
LOG_DIR="logs"
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
LOG_FILE="${LOG_DIR}/jackpot_analysis_${TIMESTAMP}.log"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
NC='\033[0m' # No Color

# Ensure log directory exists
mkdir -p "${LOG_DIR}"

# Logging function
log() {
    echo -e "${2:-}$(date '+%Y-%m-%d %H:%M:%S') - $1${NC}" | tee -a "${LOG_FILE}"
}

# Error handler
error_exit() {
    log "âŒ ERROR: $1" "${RED}"
    echo "Check log file: ${LOG_FILE}"
    exit 1
}

# Success handler
success() {
    log "âœ… $1" "${GREEN}"
}

# Info handler
info() {
    log "â„¹ï¸  $1" "${BLUE}"
}

# Warning handler
warning() {
    log "âš ï¸  $1" "${YELLOW}"
}

# Progress indicator
progress() {
    log "ðŸš€ $1" "${PURPLE}"
}

# Check prerequisites
check_prerequisites() {
    progress "Checking prerequisites..."
    
    # Check if Node.js is installed
    if ! command -v node &> /dev/null; then
        error_exit "Node.js is not installed. Please install Node.js to continue."
    fi
    success "Node.js is available"
    
    # Check if npm is installed
    if ! command -v npm &> /dev/null; then
        error_exit "npm is not installed. Please install npm to continue."
    fi
    success "npm is available"
    
    # Check for required Node modules
    if [ ! -d "node_modules" ]; then
        info "Installing Node.js dependencies..."
        npm install puppeteer-extra puppeteer-extra-plugin-stealth dotenv || error_exit "Failed to install Node.js dependencies"
        success "Node.js dependencies installed"
    fi
    
    # Check for .env file
    if [ ! -f ".env" ]; then
        warning ".env file not found. Please ensure BETIKA_USERNAME and BETIKA_PASSWORD are set."
    else
        success ".env file found"
    fi
    
    # Check for required scripts
    if [ ! -f "enhanced_jackpot_crawler.js" ]; then
        error_exit "enhanced_jackpot_crawler.js not found"
    fi
    success "Enhanced jackpot crawler script found"
    
    if [ ! -f "jackpot_prediction_analyzer.js" ]; then
        error_exit "jackpot_prediction_analyzer.js not found"
    fi
    success "Jackpot prediction analyzer script found"
}

# Clean previous runs (optional)
cleanup_previous_run() {
    progress "Cleaning up previous run data..."
    
    # Ask user if they want to clean previous data
    read -p "Do you want to clean previous run data? (y/N): " -n 1 -r
    echo
    
    if [[ $REPLY =~ ^[Yy]$ ]]; then
        info "Cleaning previous data..."
        
        # Clean directories but keep structure
        rm -rf jackpot-data/*.json match-insights/*.json screenshots/0*_*.png html-dumps/0*_*.html 2>/dev/null || true
        rm -rf predictions/*.json predictions/*.csv 2>/dev/null || true
        
        success "Previous data cleaned"
    else
        info "Keeping previous data"
    fi
}

# Run the enhanced jackpot crawler
run_crawler() {
    progress "Phase 1: Running Enhanced Jackpot Crawler..."
    
    info "Target: Crawl home, live, and jackpot pages for soccer matches"
    info "Focus: Jackpot matches beginning ${TARGET_DATE}"
    
    # Run the crawler with timeout
    timeout 1800 node enhanced_jackpot_crawler.js 2>&1 | tee -a "${LOG_FILE}" || {
        warning "Crawler finished (may have timed out after 30 minutes)"
    }
    
    # Check if crawler generated data
    if [ -f "jackpot-data/final_report.json" ]; then
        success "Crawler completed and generated data"
        
        # Show summary of collected data
        if command -v jq &> /dev/null; then
            info "Data Summary:"
            echo "  API Responses: $(jq '.summary.totalApiResponses // 0' jackpot-data/final_report.json)" | tee -a "${LOG_FILE}"
            echo "  Jackpot Items: $(jq '.summary.jackpotDataItems // 0' jackpot-data/final_report.json)" | tee -a "${LOG_FILE}"
            echo "  Match Items: $(jq '.summary.matchDataItems // 0' jackpot-data/final_report.json)" | tee -a "${LOG_FILE}"
            echo "  Odds Items: $(jq '.summary.oddsDataItems // 0' jackpot-data/final_report.json)" | tee -a "${LOG_FILE}"
            echo "  Intelligence Analyzed: $(jq '.summary.intelligenceAnalyzed // 0' jackpot-data/final_report.json)" | tee -a "${LOG_FILE}"
            echo "  External Intelligence: $(jq -r '.summary.externalIntelligence // "not_available"' jackpot-data/final_report.json)" | tee -a "${LOG_FILE}"
        fi
    else
        warning "No final report generated by crawler"
    fi
}

# Run the prediction analyzer
run_analyzer() {
    progress "Phase 2: Running Jackpot Prediction Analyzer..."
    
    info "Analyzing collected data to generate predictions for ${TARGET_DATE}"
    
    # Run the analyzer
    node jackpot_prediction_analyzer.js 2>&1 | tee -a "${LOG_FILE}" || {
        warning "Analyzer completed with warnings"
    }
    
    # Check if analyzer generated results
    if [ -d "predictions" ] && [ "$(ls -A predictions 2>/dev/null)" ]; then
        success "Analyzer completed and generated predictions"
        
        # List generated files
        info "Generated Files:"
        ls -la predictions/ | tee -a "${LOG_FILE}"
    else
        warning "No prediction files generated"
    fi
}

# Generate summary report
generate_summary() {
    progress "Phase 3: Generating Summary Report..."
    
    SUMMARY_FILE="jackpot_analysis_summary_${TIMESTAMP}.md"
    
    cat > "${SUMMARY_FILE}" << EOF
# Jackpot Analysis Summary Report
**Generated:** $(date)  
**Target Date:** ${TARGET_DATE}  
**Log File:** ${LOG_FILE}

## Execution Overview
- **Phase 1:** Enhanced Jackpot Crawler
- **Phase 2:** Prediction Analysis
- **Phase 3:** Summary Generation

## Data Collection Results
EOF

    # Add crawler results if available
    if [ -f "jackpot-data/final_report.json" ]; then
        echo "### Crawler Results" >> "${SUMMARY_FILE}"
        if command -v jq &> /dev/null; then
            echo "- API Responses Captured: $(jq '.summary.totalApiResponses // 0' jackpot-data/final_report.json)" >> "${SUMMARY_FILE}"
            echo "- Jackpot Data Items: $(jq '.summary.jackpotDataItems // 0' jackpot-data/final_report.json)" >> "${SUMMARY_FILE}"
            echo "- Match Data Items: $(jq '.summary.matchDataItems // 0' jackpot-data/final_report.json)" >> "${SUMMARY_FILE}"
            echo "- Odds Data Items: $(jq '.summary.oddsDataItems // 0' jackpot-data/final_report.json)" >> "${SUMMARY_FILE}"
        fi
        echo "" >> "${SUMMARY_FILE}"
    fi

    # Add analyzer results if available
    if [ -d "predictions" ] && [ "$(ls -A predictions 2>/dev/null)" ]; then
        echo "### Prediction Results" >> "${SUMMARY_FILE}"
        echo "Generated prediction files:" >> "${SUMMARY_FILE}"
        ls predictions/ | sed 's/^/- /' >> "${SUMMARY_FILE}"
        echo "" >> "${SUMMARY_FILE}"
        
        # Try to extract key insights
        LATEST_SUMMARY=$(ls predictions/executive_summary_*.json 2>/dev/null | tail -1)
        if [ -n "${LATEST_SUMMARY}" ] && command -v jq &> /dev/null; then
            echo "### Key Insights" >> "${SUMMARY_FILE}"
            jq -r '.keyFindings[]' "${LATEST_SUMMARY}" 2>/dev/null | sed 's/^/- /' >> "${SUMMARY_FILE}" || true
            echo "" >> "${SUMMARY_FILE}"
            
            echo "### Risk Factors" >> "${SUMMARY_FILE}"
            jq -r '.riskFactors[]' "${LATEST_SUMMARY}" 2>/dev/null | sed 's/^/- /' >> "${SUMMARY_FILE}" || true
            echo "" >> "${SUMMARY_FILE}"
            
            echo "### Next Steps" >> "${SUMMARY_FILE}"
            jq -r '.nextSteps[]' "${LATEST_SUMMARY}" 2>/dev/null | sed 's/^/- /' >> "${SUMMARY_FILE}" || true
        fi
    fi

    # Add file locations
    cat >> "${SUMMARY_FILE}" << EOF

## Generated Files and Directories
- \`jackpot-data/\` - Raw data collected by crawler
- \`match-insights/\` - Detailed match analysis
- \`predictions/\` - Prediction analysis results
- \`screenshots/\` - Page screenshots from crawler
- \`html-dumps/\` - HTML content dumps
- \`${LOG_FILE}\` - Execution log

## Next Actions
1. Review prediction files in \`predictions/\` directory
2. Analyze collected data for ${TARGET_DATE} indicators
3. Set up monitoring for identified patterns
4. Schedule regular crawls leading up to target date

EOF

    success "Summary report generated: ${SUMMARY_FILE}"
}

# Main execution function
main() {
    progress "Starting Enhanced Jackpot Analysis Workflow"
    info "Target Date: ${TARGET_DATE}"
    info "Timestamp: ${TIMESTAMP}"
    info "Log File: ${LOG_FILE}"
    
    echo "ðŸŽ¯ Enhanced Jackpot Analysis for ${TARGET_DATE}"
    echo "=================================================="
    echo ""
    
    # Execute workflow phases
    check_prerequisites
    echo ""
    
    cleanup_previous_run
    echo ""
    
    run_crawler
    echo ""
    
    run_analyzer  
    echo ""
    
    generate_summary
    echo ""
    
    success "ðŸŽ‰ Enhanced Jackpot Analysis Workflow Complete!"
    echo ""
    echo "ðŸ“Š Results:"
    echo "  - Summary Report: ${SUMMARY_FILE}"
    echo "  - Detailed Log: ${LOG_FILE}"
    echo "  - Predictions: predictions/ directory"
    echo "  - Raw Data: jackpot-data/ directory"
    echo ""
    echo "ðŸ” Next Steps:"
    echo "  1. Review ${SUMMARY_FILE} for overview"
    echo "  2. Check predictions/ for detailed analysis"
    echo "  3. Monitor for ${TARGET_DATE} jackpot announcements"
    echo "  4. Schedule regular re-runs for updated data"
}

# Handle script interruption
trap 'error_exit "Script interrupted by user"' INT TERM

# Execute main function
main "$@"